{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Основы программирования на Python - Экзамен"
      ],
      "metadata": {
        "id": "5Y1SHnawu0e0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загружаем файл для анализа по предоставленной ссылке;"
      ],
      "metadata": {
        "id": "-2moqcC5vKSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/brunoklein99/deep-learning-notes/master/shakespeare.txt"
      ],
      "metadata": {
        "id": "CCRwrYb6vRxq",
        "outputId": "597429a6-3768-4ede-c51b-7d1191ed8477",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-25 16:10:41--  https://raw.githubusercontent.com/brunoklein99/deep-learning-notes/master/shakespeare.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94275 (92K) [text/plain]\n",
            "Saving to: ‘shakespeare.txt’\n",
            "\n",
            "\rshakespeare.txt       0%[                    ]       0  --.-KB/s               \rshakespeare.txt     100%[===================>]  92.07K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2023-12-25 16:10:41 (44.8 MB/s) - ‘shakespeare.txt’ saved [94275/94275]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импортируем библиотеку nltk.\n",
        "\n",
        "В первую очередь устанавливаем пакет **popular** - он уже содержит большую часть нужных нам инструментов.\n",
        "\n",
        "Дополнительно загружаем (под)модули/функции **stopwords**, **FreqDist** и **word_tokenize**;"
      ],
      "metadata": {
        "id": "yOG5HhETvaVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('popular')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "WEw4jS1QvZnj",
        "outputId": "38f678c8-686d-493d-cef6-ba0d66f174d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Экзаменационное задание. Часть I"
      ],
      "metadata": {
        "id": "SZmaaWA9wCbq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Библиотека импортирована, файл добавлен на диск и находится в директории /content/sample_data.\n",
        "\n",
        "Теперь напишем последовательность инструкций для:\n",
        " - открытия файла\n",
        " - токенизации текста файла\n",
        " - удаления стоп-слов\n",
        " - составления частотного словаря"
      ],
      "metadata": {
        "id": "y7Qcm0e9mJ7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Откроем файл с помощью **with open** и поместим его в переменную **shake**."
      ],
      "metadata": {
        "id": "zoOtcmg0n1MI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/shakespeare.txt') as text_file:\n",
        " shake = text_file.read()"
      ],
      "metadata": {
        "id": "4WZK2jJnmJpv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Далее воспользуемся функцией **word_tokenize** для пословного извлечения токенов из текста.\n",
        "\n",
        "Посчитаем количество токенов с помощью встроенной функции **len**.\n"
      ],
      "metadata": {
        "id": "6dNw-ZIVn70x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_shake = word_tokenize(shake)\n",
        "print(len(tokenized_shake))"
      ],
      "metadata": {
        "id": "h_xuaXV6n7X8",
        "outputId": "305a1433-39b3-4289-ba5f-aa0d86f529c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20532 токенов - это много. Скорее всего, большая часть из них входит в список стоп-слов.\n",
        "\n",
        "Создадим множества стоп-слов и знаков пунктуации.\n",
        "\n",
        "Объединим названные множества и напишем цикл для их извлечения.\n",
        "\n",
        "Чтобы упростить код воспользуемся списковым включением (list comprehesion)."
      ],
      "metadata": {
        "id": "zOY2VC57p1Ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "to_remove = set(stopwords.words('english'))\n",
        "punctuation = {',', '.', '!', '-', '?', ':', ':'}\n",
        "to_remove.update(punctuation)\n",
        "ultratokenized_shake = [word for word in tokenized_shake if not word in to_remove]\n",
        "print(len(ultratokenized_shake))"
      ],
      "metadata": {
        "id": "4BPbyu4mrVqt",
        "outputId": "b64d77e4-48a2-4101-b98f-c8e26c507ac7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для частотного анализа применим класс FreqDist (frequency distributions).\n",
        "\n",
        "С помощью метода most_common получим список кортежей с самыми частыми токенами и их количеством."
      ],
      "metadata": {
        "id": "8U4eiMhPrxQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frequency_shake = FreqDist(ultratokenized_shake)\n",
        "frequency_shake.most_common(10)"
      ],
      "metadata": {
        "id": "12SCKw4orz8d",
        "outputId": "cd217f95-5cd0-4320-aeec-84c282ec730c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 351),\n",
              " ('thy', 258),\n",
              " ('And', 242),\n",
              " ('thou', 209),\n",
              " (\"'s\", 202),\n",
              " ('love', 176),\n",
              " ('thee', 161),\n",
              " ('But', 89),\n",
              " ('That', 84),\n",
              " ('doth', 82)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Экзаменационное задание. Часть II"
      ],
      "metadata": {
        "id": "rHQ0dYefwEqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь напишем функцию, которая могла бы совмещать в себе все ранее созданные \"программы\".\n",
        "\n",
        "Как и предложено, назовём функцию *word_frequency_analysis*.\n",
        "\n",
        "Для универсальности переименуем переменную **shake** в переменную **text**."
      ],
      "metadata": {
        "id": "lazkd6AGw7eM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_frequency_analysis(file_path, n=5):\n",
        "  with open(file_path) as text_file:\n",
        "    text = text_file.read()\n",
        "\n",
        "  tokenized_text = word_tokenize(text)\n",
        "\n",
        "  to_remove = set(stopwords.words('english'))\n",
        "  punctuation = {',', '.', '!', '-', '?', ':', ':'}\n",
        "  to_remove.update(punctuation)\n",
        "  ultratokenized_text = [word for word in tokenized_text if not word in to_remove]\n",
        "\n",
        "  frequency_text = FreqDist(ultratokenized_text)\n",
        "  return('Частотный словарь:', frequency_text.most_common(n))"
      ],
      "metadata": {
        "id": "j0nLSuayx_yk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/shakespeare.txt'\n",
        "word_frequency_analysis(file_path, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGlBKpZMzAfv",
        "outputId": "a67b7415-cc90-4376-b8b2-6fd5ee0960f0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Частотный словарь:',\n",
              " [('I', 351),\n",
              "  ('thy', 258),\n",
              "  ('And', 242),\n",
              "  ('thou', 209),\n",
              "  (\"'s\", 202),\n",
              "  ('love', 176),\n",
              "  ('thee', 161),\n",
              "  ('But', 89),\n",
              "  ('That', 84),\n",
              "  ('doth', 82)])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Готово!"
      ],
      "metadata": {
        "id": "pnP_oBFkztHx"
      }
    }
  ]
}